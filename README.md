# HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation

The code is designed for the paper [HoPE](https://arxiv.org/abs/2410.21216) and is built upon the modeling_llama.py in [transformers](https://github.com/huggingface/transformers).
The core implementation can be located through '## hope' and '## hope-end'


## Citation
Feel free to cite us if you like our work.
```bibtex
@inproceedings{Chen2024HoPEAN,
  title={HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation},
  author={Yuhan Chen and Ang Lv and Jian Luan and Bin Wang and Wei Liu},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273653948}
}
```


